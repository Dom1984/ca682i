{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "2.4.8 Web Scraping.ipynb",
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suzannelittle/ca682i/blob/master/notebooks/2_4_8_Web_Scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5GdG6xE-ZNN",
        "colab_type": "text"
      },
      "source": [
        "# A simple web scraping example using Python and BeautifulSoup\n",
        "\n",
        "Here's a simple example using http://books.toscrape.com/, a \"fake\" online book store setup to practise web scraping. The aim is to get a list of all the books and their prices and then save this as a JSON data structure for later use. \n",
        "\n",
        "1. Go to http://books.toscrape.com/ and use right-click \"Inspect\" (Chrome) or \"Inspect Element\" (Firefox) to see the HTML code that creates the web page.\n",
        "2. Identify the tags surrounding each book.\n",
        "3. Use requests to get the HTML of the web page.\n",
        "4. Turn the HTML into a searchable, indexable object using [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/).\n",
        "5. Find the tags for each book and extract the Title and Price values.\n",
        "6. Find the next page link and repeat steps 3-5.\n",
        "7. Save the Title & Price into a Python List and convert to a JSON string to store.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74nreExY-vjC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CX5QXTqg_mta",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = \"http://books.toscrape.com/\"\n",
        "response = requests.get(url)\n",
        "html = response.content\n",
        "soup = BeautifulSoup(html, 'lxml') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYyHBwpJBs3x",
        "colab_type": "text"
      },
      "source": [
        "Now let's find the book price and title from the article tag "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHUjaKtM_m1h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for article in soup.find_all('article'):   # find all article tags in the document\n",
        "    title = article.find('h3').find('a')['title']    # get the h3->a tag where the Title is stored in the 'title' attribute\n",
        "    price = article.find('p', {'class':'price_color'}).get_text()     # the price is in the <p class=\"price_color\"> tag\n",
        "\n",
        "    ## TODO: can you also find and include the star rating?\n",
        "\n",
        "    print((title, price))           # store the (Title, Price) text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3S-mQE7ByVj",
        "colab_type": "text"
      },
      "source": [
        "But this is just the first page! Let's find the address of the next page by getting the link from the \"next\" button."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXkn3SFL_m8o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "next = soup.find('li', {'class':'next'}).find('a')['href']\n",
        "print(next)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ul_mTKfCcVQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "next_url = url+next    # \"http://books.toscrape.com/\" + \"catalogue/page-2.html\"\n",
        "response = requests.get(next_url)\n",
        "html = response.content\n",
        "soup = BeautifulSoup(html, 'lxml')\n",
        "for article in soup.find_all('article'):   # find all article tags in the document\n",
        "    title = article.find('h3').find('a')['title']    # get the h3->a tag where the Title is stored in the 'title' attribute\n",
        "    price = article.find('p', {'class':'price_color'}).get_text()     # the price is in the <p class=\"price_color\"> tag\n",
        "    print((title, price))           # store the (Title, Price) text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viOKNO2ZC3Es",
        "colab_type": "text"
      },
      "source": [
        "Great, so we've got the next page. Let's use this and create code to loop through until the last page. This is a clumsy example of crawling a site to get all the data. The result is a list of (Title, Price) that should contain 1000 books. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_A13KMXaC8ru",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = \"http://books.toscrape.com/\"\n",
        "response = requests.get(url)\n",
        "html = response.content\n",
        "soup = BeautifulSoup(html, 'lxml')\n",
        "\n",
        "myPriceList = []\n",
        "\n",
        "for article in soup.find_all('article'): \n",
        "  title = article.find('h3').find('a')['title'] \n",
        "  price = article.find('p', {'class':'price_color'}).get_text() \n",
        "  myPriceList.append((title, price))\n",
        "\n",
        "next = soup.find('li', {'class':'next'})\n",
        "while next != None:\n",
        "  next = next.find('a')['href']\n",
        "  if not next.startswith('catalogue/'): next=\"catalogue/\"+next    # after the first link doesn't include catalogue\n",
        "  next_url = url+next   \n",
        "  print(next_url)\n",
        "  response = requests.get(next_url)\n",
        "  html = response.content\n",
        "  soup = BeautifulSoup(html, 'lxml')\n",
        "  for article in soup.find_all('article'): \n",
        "    title = article.find('h3').find('a')['title']\n",
        "    price = article.find('p', {'class':'price_color'}).get_text()\n",
        "    myPriceList.append((title, price))\n",
        "  next = soup.find('li', {'class':'next'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wimrg5LZDKF-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(myPriceList)   # should be 1000 entries ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btoY997_HTG8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "json.dumps(myPriceList)   # this could be saved to a file for later use"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzNCAEiYGf3K",
        "colab_type": "text"
      },
      "source": [
        "Hopefully this simple example shows you:\n",
        "* how useful scraping can be when there's no other way to get dynamic data\n",
        "* how complicated navigating HTML can be, especially if it's not well structured\n",
        "* how fragile scraping can be -- this code would easily break if any changes were made to the web site"
      ]
    }
  ]
}